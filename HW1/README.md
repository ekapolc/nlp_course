# TUMCUD - Total Utility Maxmimization model for Cutting of Under-segmented Data

Open your VM and download the jupyter notebook file

```
wget --no-check-certificate https://raw.githubusercontent.com/ekapolc/nlp_course/master/HW1/Word_Tokenizer_Lab.ipynb
```

**For non-VM users, download the material from [GDrive](https://drive.google.com/open?id=1iodAqVNWEkiJgH8cWkccsLi_tqoFcMrV) **

Submit the completed notebook file on MyCourseVille (4.5 points total, 0.41 points per 1 TODO)

Submit a screenshot of the closed instance (0.5 points)
